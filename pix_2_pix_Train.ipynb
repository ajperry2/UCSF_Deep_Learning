{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#If you need a new visdom server for visualization, run this command in a terminal\n",
    "# python -m visdom.server\n",
    "#then navigate here http://localhost:8097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from models import loss\n",
    "import argparse\n",
    "import os\n",
    "from util import util\n",
    "import torch\n",
    "import models\n",
    "from models import networks\n",
    "import data\n",
    "import time\n",
    "from options.train_options import TrainOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import Visualizer\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt={#BASE OPTIONS\n",
    "    #path to images (should have subfolders trainA, trainB, valA, valB, etc)\n",
    "    \"dataroot\":\"./datasets/ct_prostate_aligned/\",\n",
    "\n",
    "    \n",
    "    #'name of the experiment. It decides where to store samples and models\n",
    "    \"name\":\"new\",\n",
    "    \n",
    "    #gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\n",
    "    \"gpu_ids\":[0,1],\n",
    "    \n",
    "    # models are saved here\n",
    "    \n",
    "    \"checkpoints_dir\":\"./checkpoints_seg\",\n",
    "    \n",
    "    # chooses which model to use. [cycle_gan | pix2pix | test | colorization]\n",
    "    \"model\":\"pix2pix\",\n",
    "    \n",
    "    # number of input image channels: 3 for RGB and 1 for grayscale\n",
    "    \"input_nc\":3,\n",
    "    \n",
    "    # number of output image channels: 3 for RGB and 1 for grayscale\n",
    "    \"output_nc\":3,\n",
    "    \n",
    "    # number of gen filters in the last conv layer\n",
    "    \"ngf\":64,\n",
    "    \n",
    "    # number of discrim filters in the first conv layer\n",
    "    \"ndf\":64,\n",
    "    \n",
    "    # specify discriminator architecture [basic | n_layers | pixel]. \n",
    "    # The basic model is a 70x70 PatchGAN.\n",
    "    \"netD\":\"basic\",\n",
    "    \n",
    "    # specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]\n",
    "    # this is set in the script, changing is likely bad \n",
    "    \"netG\":\"unet_256\",\n",
    "    \n",
    "    # only used if netD==n_layers\n",
    "    \"n_layers_D\":3,\n",
    "    \n",
    "    # instance normalization or batch normalization [instance | batch | none]\n",
    "    \"norm\":\"batch\",\n",
    "    \n",
    "    # network initialization [normal | xavier | kaiming | orthogonal]\n",
    "    \"init_type\":\"normal\",\n",
    "    \n",
    "    # scaling factor for normal, xavier and orthogonal\n",
    "    \"init_gain\":0.02,\n",
    "    \n",
    "    # no dropout for the generator\n",
    "    \"no_dropout\":False,\n",
    "\n",
    "    # chooses how datasets are loaded. [unaligned | aligned | single | colorization]\n",
    "    # change to \"unaligned\" if data not already in single image pairs [AB]\n",
    "    \"dataset_mode\":\"aligned\",\n",
    "    \n",
    "    # AtoB(building to facade) or BtoA(facade to building)\n",
    "    \"direction\":\"AtoB\",\n",
    "    \n",
    "    # if true, takes images in order to make batches, otherwise takes them randomly\n",
    "    \"serial_batches\":True,\n",
    "    \n",
    "    # number of threads for loading data\n",
    "    \"num_threads\":4,\n",
    "    \n",
    "    # input batch size\n",
    "    \"batch_size\":6,\n",
    "    \n",
    "    # scale images to this size\n",
    "    \"load_size\":512,\n",
    "    \n",
    "    # then crop to this size\n",
    "    \"crop_size\":512,\n",
    "    \n",
    "    # Maximum number of samples allowed per dataset. \n",
    "    # If the dataset directory contains more than max_dataset_size, only a subset is loaded.\n",
    "    \"max_dataset_size\":float('inf'),\n",
    "    \n",
    "    # scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]\n",
    "    \"preprocess\":\"resize_and_crop\",\n",
    "    \n",
    "    # if specified, do not flip the images for data augmentation\n",
    "    \"no_flip\":\"store_true\",\n",
    "    \n",
    "    # display window size for both visdom and HTML\n",
    "    \"display_winsize\":256,\n",
    "    \n",
    "    # which epoch to load? set to latest to use latest cached model\n",
    "    \"epoch\":\"1\",\n",
    "    \n",
    "    # which iteration to load? if load_iter > 0, \n",
    "    # the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]'\n",
    "    \"load_iter\":0,\n",
    "    \n",
    "    # if specified, print more debugging information\n",
    "    \"verbose\":True,\n",
    "    \n",
    "    #customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}\n",
    "    \"suffix\":\"\",\n",
    "    \n",
    "    #TRAIN OPTIONS\n",
    "    \"isTrain\":True,\n",
    "    \n",
    "    # frequency of showing training results on screen\n",
    "    \"display_freq\":6,\n",
    "    \n",
    "    # if positive, display all images in a single visdom web panel \n",
    "    # with certain number of images per row.\n",
    "    \"display_ncols\":4,\n",
    "    \n",
    "    # window id of the web display\n",
    "    \"display_id\":1,\n",
    "    \n",
    "    # visdom server of the web display\n",
    "    \"display_server\":\"http://localhost\",\n",
    "    \n",
    "    # visdom display environment name (default is \"main\")\n",
    "    \"display_env\":\"main\",\n",
    "    \n",
    "    # visdom port of the web display\n",
    "    \"display_port\":8097,\n",
    "    \n",
    "    # frequency of saving training results to html\n",
    "    \"update_html_freq\":100,\n",
    "    \n",
    "    # frequency of showing training results on console\n",
    "    \"print_freq\":100,\n",
    "    \n",
    "    # do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/\n",
    "    \"no_html\":True,\n",
    "    \n",
    "    # network saving and loading parameters\n",
    "\n",
    "    # frequency of saving the latest results\n",
    "    \"save_latest_freq\":400,\n",
    "    \n",
    "    # frequency of saving checkpoints at the end of epochs\n",
    "    \"save_epoch_freq\":1,\n",
    "    \n",
    "    # whether saves model by iteration\n",
    "    \"save_by_iter\":True,\n",
    "    \n",
    "    # continue training: load the latest model\n",
    "    \"continue_train\":False,\n",
    "    \n",
    "    # the starting epoch count, we save the model by <epoch_count>, \n",
    "    # <epoch_count>+<save_latest_freq>, ...\n",
    "    \"epoch_count\":1,\n",
    "    \n",
    "    # train, val, test, etc\n",
    "    \"phase\":\"train\",\n",
    "    \n",
    "    # number of iter at starting learning rate\n",
    "    \"niter\":100,\n",
    "    \n",
    "    # number of iter to linearly decay learning rate to zero'\n",
    "    \"niter_decay\":100,\n",
    "    \n",
    "    # momentum term of adam\n",
    "    \"beta1\":0.5,\n",
    "    \n",
    "    # initial learning rate for adam\n",
    "    \"lr\":0.0002,\n",
    "    \n",
    "    # the type of GAN objective. [vanilla| lsgan | wgangp|dice]. \n",
    "    # vanilla GAN loss is the cross-entropy objective used in the original GAN paper.\n",
    "    \"gan_mode\":'lsgan',\n",
    "        \n",
    "    #balance classes in dice loss\n",
    "    \"balance\":True,\n",
    "    \n",
    "    # the size of image buffer that stores previously generated images\n",
    "    # this is set in the script, changing is likely bad \n",
    "    \"pool_size\":0,\n",
    "    \n",
    "    # learning rate policy. [linear | step | plateau | cosine]\n",
    "    \"lr_policy\":\"step\",\n",
    "    \n",
    "    # lambdas used for different models, only L1 is used in this case\n",
    "    'lambda_A':10.0,\n",
    "    'lambda_L1':0.0,\n",
    "    'lambda_B':10.0, \n",
    "    'lambda_identity':0.5, \n",
    "    \n",
    "    # multiply by a gamma every lr_decay_iters iterations\n",
    "    \"lr_decay_iters\":25,\n",
    "    \n",
    "    #number of classes to predict\n",
    "    \"num_classes\":6\n",
    "}\n",
    "class Map(dict):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Map, self).__init__(*args, **kwargs)\n",
    "        for arg in args:\n",
    "            if isinstance(arg, dict):\n",
    "                for k, v in arg.items():\n",
    "                    self[k] = v\n",
    "\n",
    "        if kwargs:\n",
    "            for k, v in kwargs.items():\n",
    "                self[k] = v\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return self.get(attr)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self.__setitem__(key, value)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        super(Map, self).__setitem__(key, value)\n",
    "        self.__dict__.update({key: value})\n",
    "\n",
    "    def __delattr__(self, item):\n",
    "        self.__delitem__(item)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        super(Map, self).__delitem__(key)\n",
    "        del self.__dict__[key]\n",
    "opt = Map(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "DataParallel(\n",
      "  (module): UnetGenerator(\n",
      "    (model): UnetSkipConnectionBlock(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): UnetSkipConnectionBlock(\n",
      "          (model): Sequential(\n",
      "            (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "            (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): UnetSkipConnectionBlock(\n",
      "              (model): Sequential(\n",
      "                (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (3): UnetSkipConnectionBlock(\n",
      "                  (model): Sequential(\n",
      "                    (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                    (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    (3): UnetSkipConnectionBlock(\n",
      "                      (model): Sequential(\n",
      "                        (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                        (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                        (3): UnetSkipConnectionBlock(\n",
      "                          (model): Sequential(\n",
      "                            (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                            (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                            (3): UnetSkipConnectionBlock(\n",
      "                              (model): Sequential(\n",
      "                                (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                                (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                (3): UnetSkipConnectionBlock(\n",
      "                                  (model): Sequential(\n",
      "                                    (0): LeakyReLU(negative_slope=0.2, inplace)\n",
      "                                    (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                    (2): ReLU(inplace)\n",
      "                                    (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                  )\n",
      "                                )\n",
      "                                (4): ReLU(inplace)\n",
      "                                (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                                (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                                (7): Dropout(p=0.5)\n",
      "                              )\n",
      "                            )\n",
      "                            (4): ReLU(inplace)\n",
      "                            (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                            (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                            (7): Dropout(p=0.5)\n",
      "                          )\n",
      "                        )\n",
      "                        (4): ReLU(inplace)\n",
      "                        (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                        (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                        (7): Dropout(p=0.5)\n",
      "                      )\n",
      "                    )\n",
      "                    (4): ReLU(inplace)\n",
      "                    (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                  )\n",
      "                )\n",
      "                (4): ReLU(inplace)\n",
      "                (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (4): ReLU(inplace)\n",
      "            (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): ReLU(inplace)\n",
      "        (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (4): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[Network G] Total number of parameters : 54.414 M\n",
      "DataParallel(\n",
      "  (module): NLayerDiscriminator(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[Network D] Total number of parameters : 2.769 M\n",
      "-----------------------------------------------\n",
      "saving the model at the end of epoch 1, iters 60\n",
      "learning rate = 0.0002000\n",
      "saving the model at the end of epoch 2, iters 120\n",
      "learning rate = 0.0002000\n",
      "saving the model at the end of epoch 3, iters 180\n",
      "learning rate = 0.0002000\n",
      "saving the model at the end of epoch 4, iters 240\n",
      "learning rate = 0.0002000\n",
      "(epoch: 5, iters: 60, time: 0.076, data: 0.281) G_GAN: 1.959 G_L1: 0.000 D_real: 0.415 D_fake: 0.647 \n",
      "saving the model at the end of epoch 5, iters 300\n",
      "learning rate = 0.0002000\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
    "dataset_size = len(dataset)    # get the number of images in the dataset.\n",
    "print('The number of training images = %d' % dataset_size)\n",
    "\n",
    "model = create_model(opt)      # create a model given opt.model and other options\n",
    "model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
    "visualizer = Visualizer(opt)   # create a visualizer that display/save images and plots\n",
    "total_iters = 0                # the total number of training iterations\n",
    "dice_array = []\n",
    "for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>\n",
    "    epoch_start_time = time.time()  # timer for entire epoch\n",
    "    iter_data_time = time.time()    # timer for data loading per iteration\n",
    "    epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n",
    "    dice = []\n",
    "    for i, data in enumerate(dataset):  # inner loop within one epoch\n",
    "        iter_start_time = time.time()  # timer for computation per iteration\n",
    "        if total_iters % opt.print_freq == 0:\n",
    "            t_data = iter_start_time - iter_data_time\n",
    "        visualizer.reset()\n",
    "        total_iters += opt.batch_size\n",
    "        epoch_iter += opt.batch_size\n",
    "        model.set_input(data)         # unpack data from dataset and apply preprocessing\n",
    "        \n",
    "        model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
    "        \n",
    "        \n",
    "        if total_iters % opt.display_freq == 0:   # display images on visdom and save images to a HTML file\n",
    "            \n",
    "            #pred_fake = model.fake_B\n",
    "            #pred_real = model.real_B\n",
    "            #print(loss.SoftDiceLoss(model.opt.num_classes).forward(pred_fake,pred_real))\n",
    "        \n",
    "            save_result = total_iters % opt.update_html_freq == 0\n",
    "            model.compute_visuals()\n",
    "            visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n",
    "\n",
    "        if total_iters % opt.print_freq == 0:    # print training losses and save logging information to the disk\n",
    "            losses = model.get_current_losses()\n",
    "            t_comp = (time.time() - iter_start_time) / opt.batch_size\n",
    "            visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)\n",
    "            #model.opt.gan_mode = 'dice'\n",
    "            #model.criterionGAN == networks.GANLoss(gan_mode = opt.gan_mode,num_class = opt.num_classes,options = opt)\n",
    "            \n",
    "            #real_AB = torch.cat((model.criterionGAN.real_A, model.criterionGAN.real_B), 1)\n",
    "            #pred_real = model.GANLoss.netD(real_AB)\n",
    "            \n",
    "            #fake_AB = torch.cat((model.criterionGAN.real_A, model.criterionGAN.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
    "            #pred_fake = model.GANLoss.netD(fake_AB.detach())\n",
    "            #print(loss.SoftDiceLoss().forward(pred_fake,pred_real))\n",
    "            \n",
    "            \n",
    "            if opt.display_id > 0:\n",
    "                visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, losses)\n",
    "    \n",
    "        if total_iters % opt.save_latest_freq == 0:   # cache our latest model every <save_latest_freq> iterations\n",
    "            print('saving the latest model (epoch %d, total_iters %d)' % (epoch, total_iters))\n",
    "            save_suffix = 'iter_%d' % total_iters if opt.save_by_iter else 'latest'\n",
    "            model.save_networks(save_suffix)\n",
    "\n",
    "        iter_data_time = time.time()\n",
    "    if epoch % opt.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs\n",
    "        print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))\n",
    "        model.save_networks('latest')\n",
    "        model.save_networks(epoch)\n",
    "\n",
    "    #print('End of epoch %d / %d \\t Time Taken: %d sec' % (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n",
    "    #print('Dice Score:' +str((sum(dice) / len(dice))))\n",
    "\n",
    "    #dice_array.append(str(epoch)+': '+str((sum(dice) / len(dice)))+'\\n')\n",
    "\n",
    "    model.update_learning_rate()                     # update learning rates at the end of every epoch.\n",
    "    with open('newfile.txt','w+') as f:\n",
    "            for dice_ in dice_array:\n",
    "                f.write(dice_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
